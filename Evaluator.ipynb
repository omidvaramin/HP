{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26983c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import os\n",
    "import pandas as pd\n",
    "import datasets\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "print(transformers.__version__)\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import time\n",
    "import datetime\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import numpy as np\n",
    "rougef = datasets.load_metric('rouge')\n",
    "bleu = datasets.load_metric('bleu')\n",
    "meteor = datasets.load_metric('meteor')\n",
    "print(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e87487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_add = ['/local/data1/users/omidvar/HG/BART_large_cnn/results/models/bart-large-cnn-NewsRoomSmall/checkpoint-194804','BART_largeCNN_finetuned_NS']#bart finetuned\n",
    "# model_add = ['facebook/bart-large-cnn','BART_largeCNN_pretrained_NS']\n",
    "# model_add = ['/local/data1/users/omidvar/HG/prophetnet_large_cnndm/results/models/prophetnet-large-uncased-cnndm-NewsRoom/checkpoint-97402','Prophetnet_large_cnndm_finetuned_NS']\n",
    "# model_add = ['microsoft/prophetnet-large-uncased-cnndm','Prophetnet_large_cnndm_pretrained_NS']\n",
    "# model_add = ['/local/data1/users/omidvar/HG/T5Supervised1/results/models/t5-small-NewsRoom1/checkpoint-487005', 'T5_small_finetuned_NS']\n",
    "# model_add = ['t5-small', 'T5_small_pretrained_NS']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_files = [f'/local/data1/users/omidvar/HG/Datasets/TwitterPopular/df_twitter_testvalid.feather']\n",
    "\n",
    "data_file_name = ['popular']\n",
    "run_tokenizer = True\n",
    "run_generator = True\n",
    "\n",
    "#Generator\n",
    "small_data =  ''#'' #'_small' \n",
    "filtered = '' #'_filtered' ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624d9b44",
   "metadata": {},
   "source": [
    "### Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dab5a913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenizer(run_tokenizer):\n",
    "    if not run_tokenizer:\n",
    "        return 0\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_add[0])\n",
    "    cnt = 0\n",
    "    data = pd.read_feather(data_files[0])\n",
    "    max_input_length = 1024\n",
    "    for file in tqdm(data_files):\n",
    "        data = pd.read_feather(file)\n",
    "\n",
    "        print(f'len(data) = {len(data)}')\n",
    "#         data = data.iloc[0:15]\n",
    "        text = data['target_paragraphs'].tolist()\n",
    "        text = [\"headline: \" + x for x in text]\n",
    "\n",
    "        encoding = tokenizer(text\n",
    "                             , max_length=max_input_length\n",
    "                             , truncation=True\n",
    "                             ,return_tensors=\"pt\"\n",
    "                            ,padding='longest')\n",
    "\n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_masks = encoding['attention_mask']\n",
    "        print(f'type(input_ids) = {type(input_ids)}')\n",
    "        print(f'len(input_ids) = {len(input_ids)}')\n",
    "        print(f'input_ids[0]) = {input_ids[0]}')\n",
    "        print(f'input_ids.shape  = {input_ids.shape}')\n",
    "        \n",
    "        log_path = os.path.join('./Res',model_add[1])\n",
    "        if not os.path.exists(log_path):\n",
    "            os.mkdir(log_path)\n",
    "        \n",
    "        torch.save(input_ids\n",
    "                   , os.path.join(log_path,f'input_ids_{data_file_name[cnt]}.pt'))\n",
    "        torch.save(attention_masks\n",
    "                   , os.path.join(log_path,f'attention_masks_{data_file_name[cnt]}.pt'))\n",
    "        cnt+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f5d7f9",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fedba2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_result(result, all_results_list,number_of_samples):\n",
    "    result_turn = 0\n",
    "    for i in range(len(result)):\n",
    "        all_results_list[result_turn].append(result[i])\n",
    "        \n",
    "        result_turn+=1\n",
    "        if(result_turn == number_of_samples):\n",
    "            result_turn = 0\n",
    "    return all_results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20a38348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(all_results_list\n",
    "                 , number_of_samples\n",
    "                 , file\n",
    "                 , log_path\n",
    "                ,num_beams\n",
    "                ,small_data\n",
    "                ,filtered):\n",
    "    \n",
    "    \n",
    "    #datestring = 'test'\n",
    "    \n",
    "    with open(log_path+f'/log_{file}.txt', 'w') as f:\n",
    "        f.write(f'number_of_samples = {number_of_samples}\\n')\n",
    "        \n",
    "#         f.write(f'exec_time = {exec_time} \\n')\n",
    "        f.write(f'num_beams = {num_beams} \\n')\n",
    "        f.write(f'num_beams = {small_data} \\n')\n",
    "        \n",
    "    for i in range(number_of_samples):\n",
    "        results = all_results_list[i]\n",
    "        \n",
    "        df_results = pd.DataFrame(results,columns=['Title'])\n",
    "        \n",
    "        df_results.to_csv(os.path.join(log_path, f'results_{file}{small_data}{filtered}_{i}.csv')\n",
    "                          ,index= False\n",
    "                          ,header=False)\n",
    "        df_results.reset_index().to_feather(os.path.join(log_path, f'results_{file}{small_data}{filtered}_{i}.feather'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eca4b870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(run_generator):\n",
    "    if not run_generator:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "    number_of_samples = 10\n",
    "    num_beams = 4\n",
    "    \n",
    "    small_data =  ''#'' #'_small' \n",
    "    filtered = '' #'_filtered' ''\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    datestring = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    # datestring = 'test'\n",
    "    log_path = os.path.join('./Res',model_add[1],datestring)\n",
    "    if not os.path.exists(log_path):\n",
    "        os.mkdir(log_path)\n",
    "    \n",
    "    data_path = os.path.join('./Res',model_add[1])\n",
    "    for file in tqdm(data_file_name):\n",
    "        results = []\n",
    "        input_ids = torch.load(os.path.join(data_path,f'input_ids_{file}{small_data}{filtered}.pt'))\n",
    "        attention_masks = torch.load(os.path.join(data_path,f'attention_masks_{file}{small_data}{filtered}.pt'))\n",
    "        print(f'{file} is {input_ids.shape}')\n",
    "        \n",
    "    # from transformers import T5ForConditionalGeneration\n",
    "    # model = T5ForConditionalGeneration.from_pretrained(model_add)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_add[0])\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_add[0])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for file in tqdm(data_file_name):\n",
    "    #file size should not be divisible by batch size\n",
    "        if file == 'test':\n",
    "            batch_size = 6 #3\n",
    "        else:\n",
    "            batch_size = 6 #3\n",
    "\n",
    "        all_results_list = []\n",
    "        for i in range(number_of_samples):\n",
    "            results = []\n",
    "            all_results_list.append(results)\n",
    "\n",
    "        results = []\n",
    "\n",
    "        input_ids = torch.load(os.path.join(data_path,f'input_ids_{file}{small_data}{filtered}.pt'))\n",
    "        attention_masks = torch.load(os.path.join(data_path,f'attention_masks_{file}{small_data}{filtered}.pt'))\n",
    "\n",
    "\n",
    "        #print('input_ids.device=',input_ids.device)\n",
    "        #torch.cuda.empty_cache()\n",
    "        #input_ids = input_ids[0:505]\n",
    "        #attention_masks = attention_masks[0:505]\n",
    "\n",
    "        print('input_ids.shape = ',input_ids.shape)\n",
    "\n",
    "    #     print(input_ids.shape)\n",
    "\n",
    "        batch_step = 0\n",
    "        for b in tqdm(range(int(input_ids.shape[0] / batch_size)+1)):\n",
    "            batch_step=b * batch_size\n",
    "\n",
    "            input_ids1 = input_ids[batch_step:batch_step+batch_size]\n",
    "            #print('input_ids1.shape = ', input_ids1.shape)\n",
    "            input_ids1 = input_ids1.to(device)\n",
    "            #print(batch_step,batch_step+batch_size)\n",
    "\n",
    "            attention_masks1 = attention_masks[batch_step:batch_step+batch_size]\n",
    "            attention_masks1 = attention_masks1.to(device)\n",
    "            #print('attention_masks1.shape = ', attention_masks1.shape)\n",
    "\n",
    "    #         print(f'input_ids1.shape = {input_ids1.shape}')\n",
    "    #         print(f'attention_masks1.shape = {attention_masks1.shape}')\n",
    "\n",
    "            beam_outputs = model.generate(\n",
    "                                    input_ids = input_ids1,\n",
    "                                    attention_mask = attention_masks1\n",
    "                                    ,do_sample = True\n",
    "                                    ,num_beams = num_beams\n",
    "                                    ,max_length = 20\n",
    "                                    ,min_length = 1\n",
    "                                    ,num_return_sequences = number_of_samples\n",
    "                                    #num_beams = 4,\n",
    "                                    #early_stopping = True,\n",
    "                                    #num_beam_groups = 4\n",
    "                                        )\n",
    "\n",
    "            result = tokenizer.batch_decode(beam_outputs,\n",
    "                                            skip_special_tokens=True)\n",
    "            #print('len(result)=',len(result))\n",
    "\n",
    "\n",
    "            append_result(result, all_results_list,number_of_samples)\n",
    "\n",
    "            results= results+result\n",
    " \n",
    "    #         break\n",
    "    #     break\n",
    "\n",
    "    #     print('len(results) = ',len(results))\n",
    "        end_time = time.time()\n",
    "        exec_time = end_time-start_time\n",
    "\n",
    "        save_results(all_results_list=all_results_list\n",
    "                    ,number_of_samples= number_of_samples\n",
    "                    ,file = file\n",
    "                    ,log_path = log_path\n",
    "                    ,small_data=small_data\n",
    "                    ,filtered=filtered\n",
    "                    ,num_beams = num_beams)\n",
    "    return datestring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8de6fd9",
   "metadata": {},
   "source": [
    "### Best Picker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ae688b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batch_creator(idx, all_samples, number_of_samples):\n",
    "    batch = []\n",
    "    for i in range(number_of_samples):\n",
    "        df_data = all_samples[i]\n",
    "        title = df_data.iloc[idx]['Title']\n",
    "        batch.append(title)\n",
    "    return batch\n",
    "\n",
    "def tokenizer_all(sentences, tokenizer):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for sent in sentences:\n",
    "        encoded_input = tokenizer.encode_plus(\n",
    "                                sent,\n",
    "                                add_special_tokens= True,\n",
    "                                max_length= 52,\n",
    "                                pad_to_max_length=True,\n",
    "                                return_attention_mask= True,\n",
    "                                return_tensors = 'pt')\n",
    "\n",
    "        input_ids.append(encoded_input['input_ids'])\n",
    "\n",
    "\n",
    "        attention_masks.append(encoded_input['attention_mask'])\n",
    "        \n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "        \n",
    "    return input_ids, attention_masks\n",
    "\n",
    "def bestpicker(date_time, run_bestpicker):\n",
    "    \n",
    "    if not run_bestpicker:\n",
    "        return 0\n",
    "    \n",
    "    data_address = os.path.join('./Res',model_add[1],date_time)\n",
    "    best_titles_list = []\n",
    "    best_titles_idx = []\n",
    "    all_titles_list= []\n",
    "    number_of_samples = 10\n",
    "    filetype = 'popular' #'test' #'dev'\n",
    "    filtered = '' # '' '_filtered'\n",
    "    model_address = '/local/data1/users/omidvar/HG/twitter/HQG5/Twitter/models/sentence-transformers_all-mpnet-base-v2/saved/1'\n",
    "    model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_address, # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 1, # The number of output labels--1 for regression.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "    #cache_dir= '/local/data1/omidvar/codes/HG/transformercashe'\n",
    "    )\n",
    "    model.cuda(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    all_samples = []\n",
    "    for i in range(number_of_samples):\n",
    "        data_file = os.path.join(data_address\n",
    "                                 , f'results_{filetype}{filtered}_{str(i)}.feather')\n",
    "        df_data = pd.read_feather(data_file)\n",
    "        all_samples.append(df_data)\n",
    "        \n",
    "    model.eval()    \n",
    "    \n",
    "    data_length = len(all_samples[0])\n",
    "        \n",
    "        \n",
    "    for i in tqdm(range(data_length)):\n",
    "        #read the batch\n",
    "        batch  = batch_creator(i,all_samples, number_of_samples)\n",
    "\n",
    "        #convert batch to tensors\n",
    "        input_ids, attention_masks = tokenizer_all(batch\n",
    "                                                  ,tokenizer)\n",
    "\n",
    "        #get the popularity scores\n",
    "        b_input_ids = input_ids.to(device)\n",
    "        b_attention_masks = attention_masks.to(device)\n",
    "        result = model(b_input_ids\n",
    "                      ,attention_mask = b_attention_masks)\n",
    "\n",
    "    #     if verbose:\n",
    "    #         print(result)\n",
    "        #get the best\n",
    "        largest = np.argmax(result[0].tolist())\n",
    "        best_titles_idx.append(largest)\n",
    "        #best title\n",
    "        best_title = batch[largest]\n",
    "        #add the best title\n",
    "        best_titles_list.append(best_title)\n",
    "\n",
    "\n",
    "\n",
    "        #all titles\n",
    "        for j in range(len(result[0].tolist())):\n",
    "            title = batch[j]\n",
    "            title_pop_score = result[0].tolist()[j]\n",
    "            if j == largest:\n",
    "                is_best_score = True\n",
    "            else:\n",
    "                is_best_score = False\n",
    "\n",
    "            all_titles_list.append([title, title_pop_score, is_best_score])\n",
    "        \n",
    "    #save the best results\n",
    "    df_best_titles = pd.DataFrame(best_titles_list\n",
    "                                 ,columns = ['Title'])\n",
    "    df_best_titles.to_csv(os.path.join(data_address,f'best_titles_{filetype}{filtered}.csv')\n",
    "                         ,index=False)\n",
    "    df_best_titles.to_feather(os.path.join(data_address,f'best_titles_{filetype}{filtered}.feather'))\n",
    "    \n",
    "        \n",
    "        \n",
    "    #save all titles\n",
    "    df_all_titles = pd.DataFrame(all_titles_list\n",
    "                                 ,columns = ['Title', 'title_pop_score', 'is_best_score'])\n",
    "    df_all_titles.to_csv(os.path.join(data_address,f'all_titles_{filetype}{filtered}.csv')\n",
    "                         ,index=False)\n",
    "    df_all_titles.to_feather(os.path.join(data_address,f'all_titles_{filetype}{filtered}.feather'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6379a92c",
   "metadata": {},
   "source": [
    "### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23feaddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_scoreh( predicts, references):\n",
    "    \n",
    "    \n",
    "    predicts = [pre.split(' ') for pre in predicts]\n",
    "    references = [[ref.split(' ')] for ref in references]\n",
    "    \n",
    "    score = bleu.compute(predictions=predicts,\n",
    "                references= references)\n",
    "#     print(score)\n",
    "    return score\n",
    "\n",
    "def bleu_function(pre_titles, ref_titles):\n",
    "#     print('Calculationg Bleue scores for all is began')\n",
    "    res = bleu_scoreh(pre_titles, ref_titles)\n",
    "#     print('Calculationg Bleue scores for all is done!')\n",
    "    res_list = []\n",
    "    res_list.append(res['bleu'])\n",
    "    res_list.append(res['precisions'][0])\n",
    "    res_list.append(res['precisions'][1])\n",
    "    res_list.append(res['precisions'][2])\n",
    "    res_list.append(res['precisions'][3])\n",
    "    return res_list\n",
    "#     df_res  = pd.DataFrame([res_list]\n",
    "#                             ,columns=['bleu'\n",
    "#                                     ,'bleu_1'\n",
    "#                                     ,'bleu_2'\n",
    "#                                     ,'bleu_3'\n",
    "#                                     ,'bleu_4'])\n",
    "#     df_res.to_csv(f'./Input_files/{filename}/bleuTotalscores.csv', index= False)\n",
    "  \n",
    "    \n",
    "#calculate meteor score for all\n",
    "def meteor_function(pre_titles, ref_titles):\n",
    "#     print('Meteor scores for all began')\n",
    "    res = meteor.compute(predictions = pre_titles,\n",
    "                         references = ref_titles)    \n",
    "#     print('Meteor scores for all is done!')\n",
    "    return res['meteor']\n",
    "\n",
    "    \n",
    "#calculate rouge for all 1\n",
    "def rouge_function1(pre_titles, ref_titles):\n",
    "    \n",
    "    reward = rougef.compute(predictions = pre_titles\n",
    "                            ,references = ref_titles\n",
    "                           ,rouge_types = ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
    "                           )\n",
    "    rouge1_scores = [score.fmeasure for score in reward['rouge1']]\n",
    "    rouge2_scores = [score.fmeasure for score in reward['rouge2']]\n",
    "    rougeL_scores = [score.fmeasure for score in reward['rougeL']]\n",
    "    rougeLsum_scores = [score.fmeasure for score in reward['rougeLsum']]\n",
    "    \n",
    "    rouge1_score = sum(rouge1_scores) / len(rouge1_scores)\n",
    "    rouge2_score = sum(rouge2_scores) / len(rouge2_scores)\n",
    "    rougeL_score = sum(rougeL_scores) / len(rougeL_scores)\n",
    "    rougeLsum_score = sum(rougeLsum_scores) / len(rougeLsum_scores)\n",
    "    \n",
    "#     print('rouge scores1 for all is done')\n",
    "    \n",
    "    return [rouge1_score\n",
    "            ,rouge2_score\n",
    "           ,rougeL_score\n",
    "           ,rougeLsum_score]\n",
    "#     with open(f'./Input_files/{filename}/rougeTotalscores1.txt','w') as f:\n",
    "#         print(f'rouge1 = {rouge1_score}, rouge2 = {rouge2_score}, rougeL = {rougeL_score}, rougeLsum = {rougeLsum_score}', file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0309ea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator(run_evaluator\n",
    "             , date_time\n",
    "             ,score_type = 'popular' #unpopular popular\n",
    "             ,filetype = 'popular'\n",
    "             ,fav_count_limit = 0.8 #<1.5    >0.8\n",
    "             ,max_sim_limit = 0.6\n",
    "             ):\n",
    "    \n",
    "    if not run_evaluator:\n",
    "        return 0\n",
    "    \n",
    "    ref_add = data_files[0]\n",
    "    ref_titles = pd.read_feather(ref_add)\n",
    "    \n",
    "    if score_type == 'popular':\n",
    "        data_filter = (ref_titles['q9_favorite_count_norm'] > fav_count_limit) & (ref_titles['max_sim'] < max_sim_limit)\n",
    "    elif score_type == 'unpopular':\n",
    "        data_filter = (ref_titles['q9_favorite_count_norm'] < fav_count_limit) & (ref_titles['max_sim'] < max_sim_limit)\n",
    "        \n",
    "        \n",
    "    ref_titles = ref_titles[data_filter]\n",
    "    ref_titles = ref_titles['post_text'].tolist()\n",
    "#     ref_titles = [ref_titles[0]]\n",
    "    \n",
    "    folder = os.path.join('./Res',model_add[1],date_time)\n",
    "    \n",
    "    all_scores = []\n",
    "    \n",
    "    with open(os.path.join(folder,f'all_rouge_scores_{score_type}.txt'),'w') as f:\n",
    "        set_id = 0\n",
    "        scores = []\n",
    "        scores.append(str(set_id))\n",
    "        \n",
    "        pre_titles_add = os.path.join(folder,f'results_{filetype}_{set_id}.feather')\n",
    "        pre_titles = pd.read_feather(pre_titles_add)\n",
    "        pre_titles = pre_titles[data_filter]\n",
    "        pre_titles = pre_titles['Title'].tolist()\n",
    "#             print('len(ref_titles) = ',len(ref_titles))\n",
    "#             print('len(pre_titles) = ',len(pre_titles))\n",
    "        \n",
    "        #Rouge scores\n",
    "        rouge_score = rouge_function1(pre_titles, ref_titles)\n",
    "        scores = scores + rouge_score\n",
    "        \n",
    "        #Bleu scores\n",
    "        bleu_score = bleu_function(pre_titles= pre_titles\n",
    "                                    , ref_titles= ref_titles)\n",
    "        scores = scores + bleu_score\n",
    "        \n",
    "        #Meteor\n",
    "        meteor_score = meteor_function(pre_titles= pre_titles\n",
    "                                        , ref_titles= ref_titles)\n",
    "        scores.append(meteor_score)\n",
    "#             print(len(scores))\n",
    "        all_scores.append(scores)\n",
    "        \n",
    "        f.write(f'set_id = {set_id}, rouge1_score = {rouge_score[0]} \\n')\n",
    "        \n",
    "        #getting the average of all sets\n",
    "        avg_of_sets = np.mean([x[1:] for x in all_scores]\n",
    "                              , axis=0).tolist() \n",
    "        avg_of_sets = ['avg_sets'] + avg_of_sets\n",
    "        all_scores.append(avg_of_sets)\n",
    "    \n",
    "    \n",
    "    with open(os.path.join(folder,f'all_rouge_scores_{score_type}.txt'),'a') as f: \n",
    "        scores = []\n",
    "        scores.append('best titles')\n",
    "        #best keeper\n",
    "        best_titles_add = os.path.join(folder,f'best_titles_{filetype}.feather')\n",
    "        best_titles = pd.read_feather(best_titles_add)\n",
    "        best_titles = best_titles[data_filter]\n",
    "        best_titles = best_titles['Title'].tolist()\n",
    "        #         print(len(pre_titles))\n",
    "\n",
    "        #Rouge scores\n",
    "        rouge_score = rouge_function1(best_titles, ref_titles)\n",
    "        scores = scores + rouge_score\n",
    "\n",
    "        #Bleu scores\n",
    "        bleu_score = bleu_function(pre_titles= best_titles\n",
    "                                  , ref_titles= ref_titles)\n",
    "        scores = scores + bleu_score\n",
    "\n",
    "        #Meteor\n",
    "        meteor_score = meteor_function(pre_titles= best_titles\n",
    "                                      , ref_titles= ref_titles)\n",
    "        scores.append(meteor_score)\n",
    "#         print(len(scores))\n",
    "        all_scores.append(scores)\n",
    "        f.write(f'best_titles = , rouge_score = {rouge_score} \\n')    \n",
    "    \n",
    "    \n",
    "    with open(os.path.join(folder,f'all_rouge_scores_{score_type}.txt'),'a') as f: \n",
    "        scores = []\n",
    "        scores.append('worst titles')\n",
    "        #best keeper\n",
    "        worst_titles_add = os.path.join(folder,f'worst_titles_{filetype}.feather')\n",
    "        worst_titles = pd.read_feather(worst_titles_add)\n",
    "        worst_titles = worst_titles[data_filter]\n",
    "        worst_titles = worst_titles['Title'].tolist()\n",
    "        #         print(len(pre_titles))\n",
    "\n",
    "        #Rouge scores\n",
    "        rouge_score = rouge_function1(worst_titles, ref_titles)\n",
    "        scores = scores + rouge_score\n",
    "\n",
    "        #Bleu scores\n",
    "        bleu_score = bleu_function(pre_titles= worst_titles\n",
    "                                  , ref_titles= ref_titles)\n",
    "        scores = scores + bleu_score\n",
    "\n",
    "        #Meteor\n",
    "        meteor_score = meteor_function(pre_titles= worst_titles\n",
    "                                      , ref_titles= ref_titles)\n",
    "        scores.append(meteor_score)\n",
    "#         print(len(scores))\n",
    "        all_scores.append(scores)\n",
    "        f.write(f'worst_titles = , rouge_score = {rouge_score} \\n') \n",
    "    \n",
    "    df_all_scores = pd.DataFrame(all_scores,\n",
    "                                columns=['setid'\n",
    "                                         ,'rouge1_score'\n",
    "                                        ,'rouge2_score'\n",
    "                                        ,'rougeL'\n",
    "                                        ,'rougeLSum'\n",
    "                                        ,'bleu'\n",
    "                                        ,'bleu_1'\n",
    "                                        ,'bleu_2'\n",
    "                                        ,'bleu_3'\n",
    "                                        ,'bleu_4'\n",
    "                                        ,'meteor'])\n",
    "    \n",
    "    \n",
    "    all_scores.sort(key=lambda x: x[1], reverse= False)\n",
    "    df_all_scores_add = os.path.join(folder,f'df_all_scores_{score_type}.csv')\n",
    "    df_all_scores.to_csv(df_all_scores_add, index = False)\n",
    "        \n",
    "    return all_scores, df_all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef001d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c32927",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenizer(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dafb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time = generator(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3a888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestpicker(date_time, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "632e75d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 13s, sys: 692 ms, total: 1min 13s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_rouge_scores, df_all_scores = evaluator(run_evaluator = True\n",
    "                                             , date_time = date_time\n",
    "                                             ,score_type = 'popular' #unpopular popular\n",
    "                                             ,filetype = 'popular'\n",
    "                                             ,fav_count_limit = 0.8 #<1.5    >0.8\n",
    "                                             ,max_sim_limit = 0.6\n",
    "                                             ,run_bleu = True\n",
    "                                             ,run_rouge = True\n",
    "                                             ,run_meteor= True)\n",
    "\n",
    "all_rouge_scores, df_all_scores = evaluator(run_evaluator = True\n",
    "                                             , date_time = date_time\n",
    "                                             ,score_type = 'unpopular' #unpopular popular\n",
    "                                             ,filetype = 'popular'\n",
    "                                             ,fav_count_limit = 1.5 #<1.5    >0.8\n",
    "                                             ,max_sim_limit = 0.6\n",
    "                                             ,run_bleu = True\n",
    "                                             ,run_rouge = True\n",
    "                                             ,run_meteor= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0283a5eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92825e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca9971f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
